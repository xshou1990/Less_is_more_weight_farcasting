{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "# Definition of the layers we need in the network :\n",
    "\n",
    "class LinearL1(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinearL1, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.reg_weight_decay = 1e-6\n",
    "        self.reg2_weight_decay = 0.1\n",
    "        self.register_buffer('weight_loss', torch.zeros(1))\n",
    "        self.register_buffer('bias_loss', torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.linear(x)\n",
    "\n",
    "        # Ajouter la régularisation L1 aux poids\n",
    "        self.weight_loss = torch.norm(self.linear.weight, p=1)\n",
    "        self.bias_loss = torch.norm(self.linear.bias, p=1)\n",
    "        regularizer_loss = self.reg_weight_decay * self.weight_loss + self.bias_loss*self.reg2_weight_decay\n",
    "        \n",
    "        return output, regularizer_loss\n",
    "\n",
    "\n",
    "# Lambda functions wrapped in a layer to be able to use them in the network\n",
    "# We need them to apply exp function to an output of a layer in the network\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambda_func):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambda_func = lambda_func\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.lambda_func(x)\n",
    "    \n",
    "\n",
    "# Multiplication layer to multiply two layers :\n",
    "\n",
    "class MultiplyLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiplyLayer, self).__init__()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        assert len(inputs) == 2, \"MultiplyLayer expects exactly 2 inputs\"\n",
    "        return torch.mul(inputs[0], inputs[1])\n",
    "\n",
    "# Addition layer to add two layers :\n",
    "\n",
    "class AddLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AddLayer, self).__init__()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        assert len(inputs) == 2, \"AddLayer expects exactly 2 inputs\"\n",
    "        return torch.add(inputs[0], inputs[1])\n",
    "    \n",
    "# Concatenation layer to concatenate two layers :\n",
    "\n",
    "class ConcatLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConcatLayer, self).__init__()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        assert len(inputs) == 2, \"ConcatLayer expects exactly 2 inputs\"\n",
    "        return torch.concat((inputs[0], inputs[1]), -1)\n",
    "    \n",
    "# Note that we don't need to define a constructor due to the fact there's no need to initialize any layers like in the lambda layer\n",
    "\n",
    "class WNN(nn.Module):\n",
    "    '''\n",
    "    This is the WNN model written in PyTorch. It is a weight prediction model based on the Tensorflow version of the WNN.\n",
    "    It follows the exact same architecture and layer name (that you can check on the WNN.summary() of Tensorflow).\n",
    "    \n",
    "    The model work with the function implemented in the ``WNN_PT/wnn_callback.py`` file which is the exact same function used in the \n",
    "    WeightForecasting callback class of the Tensorflow version.\n",
    "    \n",
    "    You can also check the architecture in the ``WNN_PT/Viz/model_pt.onnx.png`` and the ``WNN_PT/Viz/model_tf.onnx.png``which represents the two versions of the WNN\n",
    "    written in Tensorflow and PyTorch (it is the exact same architecture but slightly different because of the framework)\n",
    "    '''\n",
    "    def __init__(self,input_size,output_size):\n",
    "        \n",
    "        super(WNN, self).__init__()\n",
    "         \n",
    "        # We're defining the differents layers of the network here :\n",
    "        # In fact, there's only 12 differents layer throughout the network (6 dense, 4 calculus layers, the LeakyReLU layer and the ReLU one)\n",
    "        \n",
    "        \n",
    "        # Init with torch.nn.init to implement the same initialization as in the Tensorflow version\n",
    "        \n",
    "        self.dense = LinearL1(input_size, 64) # input 1 (dense n°0)\n",
    "        self.dense_6 = LinearL1(input_size-1, 64) # input 2 (dense n°6)\n",
    "        \n",
    "        self.dense_1 = nn.Linear(64, 8) \n",
    "        self.dense_2 = nn.Linear(8, 64) \n",
    "        self.dense_3 = nn.Linear(8, 64)\n",
    "        self.dense_4 = nn.Linear(8, 64) \n",
    "        self.dense_5 = LinearL1(64,32) \n",
    "\n",
    "        self.dense_7 = nn.Linear(64, 8) \n",
    "        self.dense_8 = nn.Linear(8, 64) \n",
    "        self.dense_9 = nn.Linear(8, 64) \n",
    "        self.dense_10 = nn.Linear(8, 64) \n",
    "        self.dense_11 = LinearL1(64, 32)\n",
    "        \n",
    "        self.dense_12 = nn.Linear(64, output_size) \n",
    "        \n",
    "        self.lambda_0 = LambdaLayer(lambda x: torch.exp(x)) \n",
    "        self.lambda_1 = LambdaLayer(lambda x: torch.exp(x)) \n",
    "        self.add = AddLayer() \n",
    "        self.add_1 = AddLayer() \n",
    "        self.multiply = MultiplyLayer() \n",
    "        self.multiply_1 = MultiplyLayer() \n",
    "        self.multiply_2 = MultiplyLayer() \n",
    "        self.multiply_3 = MultiplyLayer()\n",
    "        \n",
    "        self.leaky_re_lu = nn.LeakyReLU() \n",
    "        self.leaky_re_lu_1 = nn.LeakyReLU()\n",
    "        \n",
    "        self.concatenate = ConcatLayer() \n",
    "        \n",
    "        \n",
    "        # We have every layer we need for the forward propagation\n",
    "        # Note : We directly hard coded the size of inputs and outputs of each layer --> We don't need to pay attention to\n",
    "        #                                                                                the batch size pytorch will do it for us\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1 = x[0]\n",
    "        x2 = x[1]\n",
    "        # x2 = x1[0:4] - x1[1:5]\n",
    "        \n",
    "        fc1 = self.dense(x1)[0] # dense n°0\n",
    "        d1 = self.dense_1(fc1) \n",
    "        d1 = nn.ReLU()(d1) \n",
    "        d2 = self.dense_2(d1)\n",
    "        d3 = self.dense_3(d1)\n",
    "        d4 = self.dense_4(d1)\n",
    "        \n",
    "        lambd0 = self.lambda_0(self.multiply_1([fc1, d4])) \n",
    "        d5 = self.dense_5(self.add([self.multiply([d3, lambd0]), d2]))[0]\n",
    "        leaky = self.leaky_re_lu(d5)\n",
    "        \n",
    "        fc2 = self.dense_6(x2)[0] # dense n°6\n",
    "        d7 = self.dense_7(fc2)\n",
    "        d7 = nn.ReLU()(d7)\n",
    "        d8 = self.dense_8(d7)\n",
    "        d9 = self.dense_9(d7)\n",
    "        d10 = self.dense_10(d7)\n",
    "        \n",
    "        lambda1 = self.lambda_1(self.multiply_3([fc2, d10]))\n",
    "        d11 = self.dense_11(self.add_1([self.multiply_2([d9, lambda1]), d8]))[0]\n",
    "        leaky1 = self.leaky_re_lu_1(d11)\n",
    "        \n",
    "        conc = self.concatenate([leaky, leaky1])\n",
    "        out = self.dense_12(conc)\n",
    "        out = nn.Tanh()(out)\n",
    "        \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"../dataset/Least_Squares_3d_GD/Sample_1/sample.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr = data[:100,:,:]\n",
    "data_val = data[100:150,:,:]\n",
    "data_te = data[150:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_W1 = data_tr[:,:21,:]\n",
    "tr_W2 = data_tr[:,40:41,:]\n",
    "val_W1 = data_val[:,:21,:]\n",
    "val_W2 = data_val[:,40:41,:]\n",
    "te_W1 = data_te[:,:21,:]\n",
    "te_W2 = data_te[:,40:41,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_W1, tr_W2  = np.transpose(tr_W1, (0,2,1)),np.transpose(tr_W2, (0,2,1))\n",
    "val_W1, val_W2  = np.transpose(val_W1, (0,2,1)),np.transpose(val_W2, (0,2,1))\n",
    "te_W1, te_W2  = np.transpose(te_W1, (0,2,1)),np.transpose(te_W2, (0,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dX = tr_W1[:,:,:-1] - tr_W1[:,:,1:] / np.expand_dims((2 * (np.max(tr_W1, (1,2) )- np.min(tr_W1,(1,2) ))),axis=(1,2))\n",
    "tr_X = (tr_W1- np.expand_dims(tr_W1[:,:,-1],axis=2)) / np.expand_dims((2 * (np.max(tr_W1, (1,2) )- np.min(tr_W1,(1,2) ))),axis=(1,2))\n",
    "tr_y = (tr_W2- np.expand_dims(tr_W1[:,:,-1],axis=2)) / np.expand_dims((2 * (np.max(tr_W1, (1,2) )- np.min(tr_W1,(1,2) ))),axis=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dX = val_W1[:,:,:-1] - val_W1[:,:,1:] / np.expand_dims((2 * (np.max(val_W1, (1,2) )- np.min(val_W1,(1,2) ))),axis=(1,2))\n",
    "val_X = (val_W1- np.expand_dims(val_W1[:,:,-1],axis=2)) / np.expand_dims((2 * (np.max(val_W1, (1,2) )- np.min(val_W1,(1,2) ))),axis=(1,2))\n",
    "val_y = (val_W2- np.expand_dims(val_W1[:,:,-1],axis=2)) / np.expand_dims((2 * (np.max(val_W1, (1,2) )- np.min(val_W1,(1,2) ))),axis=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_dX = te_W1[:,:,:-1] - te_W1[:,:,1:] / np.expand_dims((2 * (np.max(te_W1, (1,2) )- np.min(te_W1,(1,2) ))),axis=(1,2))\n",
    "te_X = (te_W1- np.expand_dims(te_W1[:,:,-1],axis=2)) / np.expand_dims((2 * (np.max(te_W1, (1,2) )- np.min(te_W1,(1,2) ))),axis=(1,2))\n",
    "te_y = (te_W2- np.expand_dims(te_W1[:,:,-1],axis=2)) / np.expand_dims((2 * (np.max(te_W1, (1,2) )- np.min(te_W1,(1,2) ))),axis=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_size = tr_W1.shape[-1]\n",
    "output_size=tr_W2.shape[-1]\n",
    "pt_WNN = WNN(input_size = tr_W1.shape[-1], output_size=tr_W2.shape[-1]) \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.from_numpy(tr_X).float()\n",
    "dX_tensor = torch.from_numpy(tr_dX).float()\n",
    "y_tensor = torch.from_numpy(tr_y).float()\n",
    "\n",
    "val_X_tensor = torch.from_numpy(val_X).float()\n",
    "val_dX_tensor = torch.from_numpy(val_dX).float()\n",
    "val_y_tensor = torch.from_numpy(val_y).float()\n",
    "\n",
    "te_X_tensor = torch.from_numpy(te_X).float()\n",
    "te_dX_tensor = torch.from_numpy(te_dX).float()\n",
    "#te_y_tensor = torch.from_numpy(te_y).float()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 0.0026305572\n",
      "Epoch [20/1000], Loss: 0.0028059988\n",
      "Epoch [30/1000], Loss: 0.0032102072\n",
      "Epoch [40/1000], Loss: 0.0035660127\n",
      "Epoch [50/1000], Loss: 0.0052570044\n",
      "Epoch [60/1000], Loss: 0.0032333166\n",
      "Epoch [70/1000], Loss: 0.0036472220\n",
      "Epoch [80/1000], Loss: 0.0019792954\n",
      "Epoch [90/1000], Loss: 0.0019802342\n",
      "Epoch [100/1000], Loss: 0.0043692905\n",
      "Epoch [110/1000], Loss: 0.0031993557\n",
      "Epoch [120/1000], Loss: 0.0019463958\n",
      "Epoch [130/1000], Loss: 0.0024841081\n",
      "Epoch [140/1000], Loss: 0.0024112302\n",
      "Epoch [150/1000], Loss: 0.0036436450\n",
      "Epoch [160/1000], Loss: 0.0035383459\n",
      "Epoch [170/1000], Loss: 0.0018966409\n",
      "Epoch [180/1000], Loss: 0.0023068925\n",
      "Epoch [190/1000], Loss: 0.0032135688\n",
      "Epoch [200/1000], Loss: 0.0031546608\n",
      "Epoch [210/1000], Loss: 0.0012166785\n",
      "Epoch [220/1000], Loss: 0.0022658010\n",
      "Epoch [230/1000], Loss: 0.0024332425\n",
      "Epoch [240/1000], Loss: 0.0019080350\n",
      "Epoch [250/1000], Loss: 0.0039159362\n",
      "Epoch [260/1000], Loss: 0.0023111321\n",
      "Epoch [270/1000], Loss: 0.0030870249\n",
      "Epoch [280/1000], Loss: 0.0020031324\n",
      "Epoch [290/1000], Loss: 0.0014885435\n",
      "Epoch [300/1000], Loss: 0.0049931486\n",
      "Epoch [310/1000], Loss: 0.0027106032\n",
      "Epoch [320/1000], Loss: 0.0026639237\n",
      "Epoch [330/1000], Loss: 0.0029495088\n",
      "Epoch [340/1000], Loss: 0.0037155382\n",
      "Epoch [350/1000], Loss: 0.0038351808\n",
      "Epoch [360/1000], Loss: 0.0064010001\n",
      "Epoch [370/1000], Loss: 0.0036771297\n",
      "Epoch [380/1000], Loss: 0.0033716892\n",
      "Epoch [390/1000], Loss: 0.0071202233\n",
      "Epoch [400/1000], Loss: 0.0019088314\n",
      "Epoch [410/1000], Loss: 0.0022633818\n",
      "Epoch [420/1000], Loss: 0.0032466853\n",
      "Epoch [430/1000], Loss: 0.0013681557\n",
      "Epoch [440/1000], Loss: 0.0019063771\n",
      "Epoch [450/1000], Loss: 0.0025533696\n",
      "Epoch [460/1000], Loss: 0.0024492040\n",
      "Epoch [470/1000], Loss: 0.0015785643\n",
      "Epoch [480/1000], Loss: 0.0018741029\n",
      "Epoch [490/1000], Loss: 0.0045328126\n",
      "Epoch [500/1000], Loss: 0.0040418240\n",
      "Epoch [510/1000], Loss: 0.0028312765\n",
      "Epoch [520/1000], Loss: 0.0031778626\n",
      "Epoch [530/1000], Loss: 0.0038231805\n",
      "Epoch [540/1000], Loss: 0.0033706650\n",
      "Epoch [550/1000], Loss: 0.0019736795\n",
      "Epoch [560/1000], Loss: 0.0045577241\n",
      "Epoch [570/1000], Loss: 0.0025315690\n",
      "Epoch [580/1000], Loss: 0.0034877493\n",
      "Epoch [590/1000], Loss: 0.0029518101\n",
      "Epoch [600/1000], Loss: 0.0020306006\n",
      "Epoch [610/1000], Loss: 0.0019067622\n",
      "Epoch [620/1000], Loss: 0.0032814313\n",
      "Epoch [630/1000], Loss: 0.0036450892\n",
      "Epoch [640/1000], Loss: 0.0023284683\n",
      "Epoch [650/1000], Loss: 0.0017098892\n",
      "Epoch [660/1000], Loss: 0.0015682004\n",
      "Epoch [670/1000], Loss: 0.0025363232\n",
      "Epoch [680/1000], Loss: 0.0020209320\n",
      "Epoch [690/1000], Loss: 0.0032698147\n",
      "Epoch [700/1000], Loss: 0.0031997364\n",
      "Epoch [710/1000], Loss: 0.0014379359\n",
      "Epoch [720/1000], Loss: 0.0023528300\n",
      "Epoch [730/1000], Loss: 0.0024655724\n",
      "Epoch [740/1000], Loss: 0.0033261443\n",
      "Epoch [750/1000], Loss: 0.0026562356\n",
      "Epoch [760/1000], Loss: 0.0026660536\n",
      "Epoch [770/1000], Loss: 0.0039786757\n",
      "Epoch [780/1000], Loss: 0.0023397163\n",
      "Epoch [790/1000], Loss: 0.0026424443\n",
      "Epoch [800/1000], Loss: 0.0037057877\n",
      "Epoch [810/1000], Loss: 0.0043670279\n",
      "Epoch [820/1000], Loss: 0.0016468951\n",
      "Epoch [830/1000], Loss: 0.0028573219\n",
      "Epoch [840/1000], Loss: 0.0020442705\n",
      "Epoch [850/1000], Loss: 0.0039918711\n",
      "Epoch [860/1000], Loss: 0.0022443065\n",
      "Epoch [870/1000], Loss: 0.0024399646\n",
      "Epoch [880/1000], Loss: 0.0037464350\n",
      "Epoch [890/1000], Loss: 0.0013993169\n",
      "Epoch [900/1000], Loss: 0.0014562231\n",
      "Epoch [910/1000], Loss: 0.0017961026\n",
      "Epoch [920/1000], Loss: 0.0023800049\n",
      "Epoch [930/1000], Loss: 0.0010315479\n",
      "Epoch [940/1000], Loss: 0.0041376078\n",
      "Epoch [950/1000], Loss: 0.0022124739\n",
      "Epoch [960/1000], Loss: 0.0043233656\n",
      "Epoch [970/1000], Loss: 0.0025182108\n",
      "Epoch [980/1000], Loss: 0.0012484445\n",
      "Epoch [990/1000], Loss: 0.0023830782\n",
      "Epoch [1000/1000], Loss: 0.0043421513\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "# X_tensor = torch.from_numpy(tr_W1).float()\n",
    "# y_tensor = torch.from_numpy(tr_W2).float()\n",
    "\n",
    "# ### need to check if this is true\n",
    "# target = (y_tensor- X_tensor[:,:,0:1]) /torch.unsqueeze((2 * (torch.max(y_tensor,1).values - torch.min(y_tensor,1).values)).repeat(1,3),dim=2)\n",
    "\n",
    "# maeloss = nn.L1Loss()\n",
    "\n",
    "learn_rate = 0.0001\n",
    "#batchsize\n",
    "batch_size = 32\n",
    "\n",
    "# Loss and optimizer\n",
    "optimizer = optim.Adam(pt_WNN.parameters(), lr=learn_rate)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 1000\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    indices = torch.randperm(len(X_tensor))\n",
    "\n",
    "    for i in range(0, len(X_tensor), batch_size):\n",
    "        batch_indices = indices[i:i+batch_size]\n",
    "        outputs = pt_WNN( [X_tensor[batch_indices], dX_tensor[batch_indices]] )\n",
    "    \n",
    "        loss = torch.squeeze(torch.mean(torch.abs(outputs - y_tensor[batch_indices])))\n",
    "        #print(loss)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.10f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted =  np.squeeze(pt_WNN([te_X_tensor, te_dX_tensor] ).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = predicted * (np.expand_dims((2 * (np.max(te_W1, (1,2) )- np.min(te_W1,(1,2) ))),axis=1)) * 2 + te_W1[:,:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025676319986553158"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(pred_y - np.squeeze(te_W2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
