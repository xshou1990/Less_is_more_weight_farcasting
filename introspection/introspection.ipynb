{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"../dataset/Least_Squares_3d_GD/Sample_1/sample.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr = data[:100,:,:]\n",
    "data_val = data[100:150,:,:]\n",
    "data_te = data[150:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_n(data,batch_size,k):\n",
    "#     if data.shape[2] > 20:\n",
    "#         outputb = np.zeros((batch_size,4, 20))\n",
    "#         outputl = np.zeros((batch_size,1, 20))\n",
    "#     else: \n",
    "    outputb = np.zeros((batch_size,4, data.shape[2]))\n",
    "    outputl = np.zeros((batch_size,180, data.shape[2]))\n",
    "    indices = np.random.permutation(len(data))\n",
    "    for i in range(batch_size):\n",
    "        index = indices[i]\n",
    "        b,l = get_batch(data[index,:,:],k)\n",
    "        outputb[i,:,:] = b\n",
    "        outputl[i,:,:] = l\n",
    "        \n",
    "    return outputb, outputl\n",
    "\n",
    "\n",
    "def get_batch(train_data, k):\n",
    "    \n",
    "    # since we predict 2t, make sure all ts have corresponding tragets.\n",
    "    maxval = train_data.shape[0] / 2\n",
    "    data_width = train_data.shape[1]\n",
    "    data_0 = train_data[0, :]\n",
    "\n",
    "    \n",
    "#    batch_indices = np.zeros(20, dtype=np.int32)\n",
    "#     t = np.random.randint(low=1, high=maxval)\n",
    "\n",
    "    t = 20 # now predict k*t\n",
    "\n",
    "    t_1 = int(7 * t / 10)\n",
    "    t_2 = int(4 * t / 10)\n",
    "    data_t = train_data[t, :]\n",
    "    data_t_1 = train_data[t_1, :]\n",
    "    data_t_2 = train_data[t_2, :]\n",
    "\n",
    "#     if data_width > 20:\n",
    "#         diff = np.abs(data_0 - data_t)\n",
    "#         sorted_diff = np.argsort(diff)\n",
    "#         p_50_100 = sorted_diff[int(data_width / 2): ]\n",
    "#         p_25_50 = sorted_diff[int(data_width / 4): int(data_width / 2)]\n",
    "#         p_0_25 = sorted_diff[: int(data_width / 4)]\n",
    "\n",
    "#         batch_indices[:10] = np.random.choice(p_50_100, 10, replace=False)\n",
    "#         batch_indices[10:15] = np.random.choice(p_25_50, 5, replace=False)\n",
    "#         batch_indices[15:] = np.random.choice(p_0_25, 5, replace=False)\n",
    "#     else:\n",
    "    batch_indices = np.arange(data_width)\n",
    "    \n",
    "    batch_t = np.take(data_t, batch_indices)\n",
    "\n",
    "    batch_t_1 = np.take(data_t_1, batch_indices)\n",
    "    batch_t_2 = np.take(data_t_2, batch_indices)\n",
    "    batch_t_0 = np.take(data_0, batch_indices)\n",
    "    labels = np.take(train_data[21:, :], batch_indices)[np.newaxis, :]\n",
    "#     labels = train_data[2:, :]\n",
    "    batch = np.vstack((batch_t, batch_t_1))\n",
    "    batch = np.vstack((batch, batch_t_2))\n",
    "    batch = np.vstack((batch, batch_t_0))\n",
    "\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "tr_W1, tr_W2 = get_batch_n(data_tr,len(data_tr), k)\n",
    "tr_W1, tr_W2  = np.transpose(tr_W1, (0,2,1)),np.transpose(tr_W2, (0,2,1))\n",
    "val_W1, val_W2 = get_batch_n(data_val,len(data_val),k)\n",
    "val_W1, val_W2  = np.transpose(val_W1, (0,2,1)),np.transpose(val_W2, (0,2,1))\n",
    "te_W1, te_W2 = get_batch_n(data_te,len(data_te),k)\n",
    "te_W1, te_W2  = np.transpose(te_W1, (0,2,1)),np.transpose(te_W2, (0,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 3, 4), (100, 3, 180))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_W1.shape, tr_W2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000], Train Loss: 0.4349789023, val Loss: 0.4353388250 \n",
      "Epoch [2/10000], Train Loss: 0.3543502688, val Loss: 0.3583093882 \n",
      "Epoch [3/10000], Train Loss: 0.2900410891, val Loss: 0.2967842519 \n",
      "Epoch [4/10000], Train Loss: 0.2375498116, val Loss: 0.2461919188 \n",
      "Epoch [5/10000], Train Loss: 0.1953577548, val Loss: 0.2053975463 \n",
      "Epoch [6/10000], Train Loss: 0.1623090655, val Loss: 0.1733477116 \n",
      "Epoch [7/10000], Train Loss: 0.1375719756, val Loss: 0.1490115672 \n",
      "Epoch [8/10000], Train Loss: 0.1199484766, val Loss: 0.1314902157 \n",
      "Epoch [9/10000], Train Loss: 0.1067191809, val Loss: 0.1179156750 \n",
      "Epoch [10/10000], Train Loss: 0.0949919149, val Loss: 0.1057578549 \n",
      "Epoch [11/10000], Train Loss: 0.0855749622, val Loss: 0.0957178026 \n",
      "Epoch [12/10000], Train Loss: 0.0794789344, val Loss: 0.0889550149 \n",
      "Epoch [13/10000], Train Loss: 0.0748500824, val Loss: 0.0839937627 \n",
      "Epoch [14/10000], Train Loss: 0.0703479573, val Loss: 0.0791821107 \n",
      "Epoch [15/10000], Train Loss: 0.0657822639, val Loss: 0.0743900537 \n",
      "Epoch [16/10000], Train Loss: 0.0618912354, val Loss: 0.0706798583 \n",
      "Epoch [17/10000], Train Loss: 0.0589344986, val Loss: 0.0675387830 \n",
      "Epoch [18/10000], Train Loss: 0.0555786528, val Loss: 0.0637713000 \n",
      "Epoch [19/10000], Train Loss: 0.0525153689, val Loss: 0.0603878275 \n",
      "Epoch [20/10000], Train Loss: 0.0500826873, val Loss: 0.0575579666 \n",
      "Epoch [21/10000], Train Loss: 0.0479166545, val Loss: 0.0550264232 \n",
      "Epoch [22/10000], Train Loss: 0.0457601994, val Loss: 0.0525842011 \n",
      "Epoch [23/10000], Train Loss: 0.0438477732, val Loss: 0.0504020043 \n",
      "Epoch [24/10000], Train Loss: 0.0420527533, val Loss: 0.0484986790 \n",
      "Epoch [25/10000], Train Loss: 0.0403848924, val Loss: 0.0464755595 \n",
      "Epoch [26/10000], Train Loss: 0.0392064974, val Loss: 0.0448635481 \n",
      "Epoch [27/10000], Train Loss: 0.0373807885, val Loss: 0.0431298018 \n",
      "Epoch [28/10000], Train Loss: 0.0359822139, val Loss: 0.0414962731 \n",
      "Epoch [29/10000], Train Loss: 0.0345288962, val Loss: 0.0399499387 \n",
      "Epoch [30/10000], Train Loss: 0.0330715664, val Loss: 0.0381441414 \n",
      "Epoch [31/10000], Train Loss: 0.0316572152, val Loss: 0.0365970880 \n",
      "Epoch [32/10000], Train Loss: 0.0303549822, val Loss: 0.0350733362 \n",
      "Epoch [33/10000], Train Loss: 0.0290006548, val Loss: 0.0334080867 \n",
      "Epoch [34/10000], Train Loss: 0.0277868975, val Loss: 0.0319330841 \n",
      "Epoch [35/10000], Train Loss: 0.0264330786, val Loss: 0.0304038040 \n",
      "Epoch [36/10000], Train Loss: 0.0253239740, val Loss: 0.0289642923 \n",
      "Epoch [37/10000], Train Loss: 0.0247384161, val Loss: 0.0283891018 \n",
      "Epoch [38/10000], Train Loss: 0.0235693268, val Loss: 0.0270172209 \n",
      "Epoch [39/10000], Train Loss: 0.0219859704, val Loss: 0.0250304695 \n",
      "Epoch [40/10000], Train Loss: 0.0208764281, val Loss: 0.0238477718 \n",
      "Epoch [41/10000], Train Loss: 0.0198348872, val Loss: 0.0226483829 \n",
      "Epoch [42/10000], Train Loss: 0.0190572087, val Loss: 0.0216137450 \n",
      "Epoch [43/10000], Train Loss: 0.0177329909, val Loss: 0.0200929604 \n",
      "Epoch [44/10000], Train Loss: 0.0166582242, val Loss: 0.0189767983 \n",
      "Epoch [45/10000], Train Loss: 0.0159442574, val Loss: 0.0181308314 \n",
      "Epoch [46/10000], Train Loss: 0.0152290119, val Loss: 0.0171328764 \n",
      "Epoch [47/10000], Train Loss: 0.0141758593, val Loss: 0.0159273464 \n",
      "Epoch [48/10000], Train Loss: 0.0134827429, val Loss: 0.0151650459 \n",
      "Epoch [49/10000], Train Loss: 0.0127091920, val Loss: 0.0143109020 \n",
      "Epoch [50/10000], Train Loss: 0.0120430747, val Loss: 0.0134140151 \n",
      "Epoch [51/10000], Train Loss: 0.0110340528, val Loss: 0.0123201711 \n",
      "Epoch [52/10000], Train Loss: 0.0103042852, val Loss: 0.0115539972 \n",
      "Epoch [53/10000], Train Loss: 0.0098845121, val Loss: 0.0110152485 \n",
      "Epoch [54/10000], Train Loss: 0.0095477924, val Loss: 0.0105872937 \n",
      "Epoch [55/10000], Train Loss: 0.0092759263, val Loss: 0.0101276617 \n",
      "Epoch [56/10000], Train Loss: 0.0090870121, val Loss: 0.0098524112 \n",
      "Epoch [57/10000], Train Loss: 0.0083804503, val Loss: 0.0090979245 \n",
      "Epoch [58/10000], Train Loss: 0.0079138381, val Loss: 0.0085894307 \n",
      "Epoch [59/10000], Train Loss: 0.0071536419, val Loss: 0.0077472683 \n",
      "Epoch [60/10000], Train Loss: 0.0068879798, val Loss: 0.0073729702 \n",
      "Epoch [61/10000], Train Loss: 0.0066475836, val Loss: 0.0071006655 \n",
      "Epoch [62/10000], Train Loss: 0.0065289983, val Loss: 0.0068930951 \n",
      "Epoch [63/10000], Train Loss: 0.0067792381, val Loss: 0.0070513301 \n",
      "Epoch [64/10000], Train Loss: 0.0063734669, val Loss: 0.0066883988 \n",
      "Epoch [65/10000], Train Loss: 0.0059527159, val Loss: 0.0062024277 \n",
      "Epoch [66/10000], Train Loss: 0.0056955810, val Loss: 0.0058689462 \n",
      "Epoch [67/10000], Train Loss: 0.0056468640, val Loss: 0.0058119190 \n",
      "Epoch [68/10000], Train Loss: 0.0052783294, val Loss: 0.0054037231 \n",
      "Epoch [69/10000], Train Loss: 0.0052095093, val Loss: 0.0053169625 \n",
      "Epoch [70/10000], Train Loss: 0.0046207919, val Loss: 0.0047074128 \n",
      "Epoch [71/10000], Train Loss: 0.0045520482, val Loss: 0.0046057845 \n",
      "Epoch [72/10000], Train Loss: 0.0044232430, val Loss: 0.0044661155 \n",
      "Epoch [73/10000], Train Loss: 0.0039110905, val Loss: 0.0039506811 \n",
      "Epoch [74/10000], Train Loss: 0.0039107632, val Loss: 0.0039139572 \n",
      "Epoch [75/10000], Train Loss: 0.0040258416, val Loss: 0.0040009716 \n",
      "Epoch [76/10000], Train Loss: 0.0035957231, val Loss: 0.0035733823 \n",
      "Epoch [77/10000], Train Loss: 0.0037142136, val Loss: 0.0036779260 \n",
      "Epoch [78/10000], Train Loss: 0.0036471332, val Loss: 0.0036089956 \n",
      "Epoch [79/10000], Train Loss: 0.0035303831, val Loss: 0.0034865271 \n",
      "Epoch [80/10000], Train Loss: 0.0037103780, val Loss: 0.0036608055 \n",
      "Epoch [81/10000], Train Loss: 0.0040440741, val Loss: 0.0039713252 \n",
      "Epoch [82/10000], Train Loss: 0.0036952896, val Loss: 0.0036314940 \n",
      "Epoch [83/10000], Train Loss: 0.0038730253, val Loss: 0.0038027847 \n",
      "Epoch [84/10000], Train Loss: 0.0036663960, val Loss: 0.0036044144 \n",
      "Breaking due to early stopping at epoch 83\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_tensor = torch.from_numpy(tr_W1).float()\n",
    "y_tensor = torch.from_numpy(tr_W2).float()\n",
    "\n",
    "X_tensor_val = torch.from_numpy(val_W1).float()\n",
    "y_tensor_val = torch.from_numpy(val_W2).float()\n",
    "\n",
    "X_tensor_te = torch.from_numpy(te_W1).float()\n",
    "y_tensor_te = torch.from_numpy(te_W2).float()\n",
    "\n",
    "# Define the linear regression model\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size,output_size):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "input_size = tr_W1.shape[2]\n",
    "\n",
    "output_size = tr_W2.shape[2]\n",
    "\n",
    "learn_rate = 0.01\n",
    "\n",
    "model = LinearRegressionModel(input_size,output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learn_rate)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 10000\n",
    "\n",
    "#batchsize\n",
    "batch_size = 32\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model = deepcopy(model.state_dict())\n",
    "val_loss_list = [] \n",
    "impatience = 0 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    indices = torch.randperm(len(X_tensor))\n",
    "\n",
    "    for i in range(0, len(X_tensor), batch_size):\n",
    "        batch_indices = indices[i:i+batch_size]\n",
    "    \n",
    "        outputs = model( X_tensor[batch_indices])\n",
    "        \n",
    "        # Compute the loss : \n",
    "        loss = torch.squeeze(torch.mean(torch.abs(outputs - y_tensor[batch_indices])))\n",
    "        \n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    tr_loss = torch.squeeze(torch.mean(torch.abs(model(X_tensor) - y_tensor)))\n",
    "    val_loss= torch.squeeze(torch.mean(torch.abs(model(X_tensor_val) - y_tensor_val)))\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {tr_loss.item():.10f}, val Loss: {val_loss.item():.10f} ')\n",
    "\n",
    "    if  val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = deepcopy(model.state_dict())\n",
    "            impatient = 0 # reset\n",
    "    else:\n",
    "        impatient += 1\n",
    "\n",
    "\n",
    "    if impatient >= 5:\n",
    "        print(f'Breaking due to early stopping at epoch {epoch}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse of test samples 8.436880307272077e-06 at predicting time step 60 \n",
      "mse of test samples 7.365811597992433e-06 at predicting time step 80 \n",
      "mse of test samples 2.9596763852168806e-05 at predicting time step 100 \n",
      "mse of test samples 1.1715062555595068e-06 at predicting time step 120 \n",
      "mse of test samples 4.9684194891597144e-06 at predicting time step 140 \n",
      "mse of test samples 1.7018983271555044e-05 at predicting time step 160 \n",
      "mse of test samples 0.00015621347120031714 at predicting time step 180 \n",
      "mse of test samples 2.4398295863647945e-06 at predicting time step 200 \n"
     ]
    }
   ],
   "source": [
    "# # evaluate\n",
    "# model.load_state_dict(best_model)\n",
    "    \n",
    "# model.eval()\n",
    "# mse = torch.mean(torch.squeeze(torch.square(model(X_tensor_te) - y_tensor_te)))\n",
    "\n",
    "# print(\"mse of test samples {} at predicting time step {} \".format(mse, k*20 ) )\n",
    "\n",
    "# evaluate\n",
    "model.load_state_dict(best_model)\n",
    "    \n",
    "model.eval()\n",
    "\n",
    "pred = model(X_tensor_te)\n",
    "\n",
    "for k in [3,4,5,6,7,8,9,10]:\n",
    "    mse = torch.mean(torch.squeeze(torch.square(pred[:,:,20*(k-1)-1] - y_tensor_te[:,:,20*(k-1)-1])))\n",
    "\n",
    "    print(\"mse of test samples {} at predicting time step {} \".format(mse, k*20 ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform experiments: using 4 points 0, 0.4t 0.7t, t -> 2t\n",
    "# exp1. t= 20, 0,8,14,20 -> 40\n",
    "# exp2. t =50, 0, 20, 35, 50 -> 100\n",
    "# exp3. t= 100, 0,40,70, 100, -> 200\n",
    "\n",
    "# Introspection: scheme 1\n",
    "# exp1. t= 20, 0,8,14,20 -> 40\n",
    "# exp2. t =20, 0,8,14,20 -> 100\n",
    "# exp3. t= 20,0,8,14,20 -> 200\n",
    "\n",
    "# WNN:  -> 40 \n",
    "# exp1. 30-35 - > \n",
    "# exp2. 90-95 -> \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
